# Publication-Ready Methodology (English)

## Abstract
We present an offline-to-online reinforcement learning pipeline for AGV warehouse navigation: expert trajectory generation using classical planning/control, offline policy initialization via TD3-BC, online adaptation via TD3, and constrained LLM-driven outer-loop tuning of hierarchical potential-based reward shaping (HPRS) constants. The design prioritizes interpretability, auditability, and safety-aware performance.

## 1. Problem Formulation
Warehouse navigation is modeled as a continuous-control MDP:
$$
\mathcal{M}=(\mathcal{S},\mathcal{A},P,r,\gamma).
$$
State includes robot pose/velocity, goal-relative terms, and optional laser observations from Unity; action is continuous chassis linear/angular velocity. The objective is to maximize task completion under safety constraints (collision minimization).

Implementation mapping:
- Environment: `agent/envs/warehouse_unity_env.py`
- Vectorized simulator bridge: `agent/simulator_vec_env.py`
- Environment assembly: `agent/envs/get_env.py`

## 2. Staged Training Pipeline

### 2.1 Expert Data Collection (A* + DWA)
For each parallel environment, a global path is planned (A*) and local control actions are generated by DWA/P-controller. Collected transitions:
$$
\mathcal{D}=\{(s_t,a_t,r_t,s_{t+1},d_t)\}_{t=1}^{N}.
$$
Only successful episodes are retained to improve offline dataset quality.

Implementation: `agent/expert/run_expert.py`

### 2.2 Offline TD3-BC Pretraining
The offline stage combines TD3 critics with behavior cloning regularization.

TD target:
$$
y_t = r_t + \gamma (1-d_t)\min_{i=1,2}Q_{\bar\phi_i}(s_{t+1}, \tilde a_{t+1}),
$$
with target policy smoothing:
$$
\tilde a_{t+1}=\text{clip}\left(\pi_{\bar\theta}(s_{t+1})+\epsilon,\,-a_{\max},a_{\max}\right),\quad
\epsilon\sim\text{clip}(\mathcal{N}(0,0.05^2),-0.2,0.2).
$$

Critic loss:
$$
\mathcal{L}_{critic}=\sum_{i=1}^{2}\left\|Q_{\phi_i}(s_t,a_t)-y_t\right\|_2^2.
$$

Actor loss (with delayed update):
$$
\lambda = \frac{\alpha_{\text{refined}}}{\mathbb{E}_{s\sim\mathcal D}[|Q_{\phi_1}(s,\pi_\theta(s))|]},
$$
$$
\mathcal{L}_{actor}= -\lambda\cdot \mathbb{E}[Q_{\phi_1}(s,\pi_\theta(s))]
\;+\;
\mathbb{E}\left[\|\pi_\theta(s)-a\|_2^2\right].
$$
where $\alpha_{\text{refined}}=\alpha\cdot\text{policy\_refinement\_factor}$.

Implementation: `agent/td3/td3bc/td3_bc_agent.py`, `agent/td3/td3bc/train_td3bc.py`

### 2.3 Online TD3 Finetuning (Warm Start)
Online training initializes actor/critic from offline checkpoints and continues environment interaction to adapt policy under online data distribution.

Implementation: `agent/td3/online_td3/train_online_td3.py`

## 3. Egocentric Observation and Action Scaling
To stabilize learning, world-frame observations are transformed into egocentric polar features:
$$
d=\sqrt{\Delta x^2+\Delta y^2},\quad
\beta=\text{atan2}(\Delta y,\Delta x)-\psi.
$$
Navigation features:
$$
\left[\frac{d}{d_{\max}},\sin\beta,\cos\beta,\frac{v}{v_{\max}},\frac{\omega}{\omega_{\max}}\right],
$$
plus normalized lidar channels when enabled.

Policy outputs actions in $[-1,1]^2$, then denormalized before execution:
$$
a_{\text{phys}}=\frac{a+1}{2}\odot(a_{\max}-a_{\min})+a_{\min}.
$$

Implementation: `agent/td3/online_td3/egocentric_normalization_wrapper.py`

## 4. HPRS: Hierarchical Potential-Based Reward Shaping

### 4.1 Hierarchical Semantics
Reward constraints are organized by priority:
1. Safety (`ensure`)
2. Target (`achieve`/`conquer`)
3. Comfort (`encourage`)

Specs are declared in YAML and parsed into executable predicates.

Implementation: `auto-shaping/configs/warehouse.yaml`, `auto-shaping/auto_shaping/spec/reward_spec.py`

### 4.2 Predicate Normalization
Define:
$$
\text{norm}(v; l,u)=\frac{\text{clip}(v,l,u)-l}{u-l}\in[0,1].
$$
For predicate $v\le \tau$:
$$
\rho(v;\tau)=1-\text{norm}(v;\tau,u),
$$
for $v\ge \tau$:
$$
\rho(v;\tau)=\text{norm}(v;l,\tau).
$$

Bounds $(l,u)$ come from variable ranges in the HPRS spec.

### 4.3 Potential Functions
Safety mask:
$$
m_s(s)=\prod_{k\in\mathcal{S}}\mathbf{1}[g_k(s)].
$$

Target potential:
$$
\Phi_t(s)=m_s(s)\cdot\frac{1}{|\mathcal{T}|}\sum_{k\in\mathcal{T}}\rho_k(s).
$$

Safety potential (as implemented: count of satisfied ensure predicates):
$$
\Phi_s(s)=\sum_{k\in\mathcal{S}}\mathbf{1}[g_k(s)].
$$

Comfort potential (soft-gated by target quality):
$$
m_t(s)=\frac{1}{|\mathcal{T}|}\sum_{k\in\mathcal{T}}\rho_k(s),\quad
\Phi_c(s)=m_s(s)\,m_t(s)\sum_{j\in\mathcal{C}}\rho_j(s).
$$

### 4.4 Final Reward
Sparse base success reward:
$$
r_{\text{base}}(s')=\mathbf{1}[\text{success}(s')].
$$

Potential-difference shaping (for non-terminal transitions with valid previous state):
$$
r_{\text{pot}} = r_{\text{base}}
\lambda_{\text{shape}}\Big(
\gamma\Phi_s(s')-\Phi_s(s)
\gamma\Phi_t(s')-\Phi_t(s)
\gamma\Phi_c(s')-\Phi_c(s)
\Big).
$$

Two additional dense terms are included in implementation:
$$
r_{\Delta d}=w_{\Delta d}\cdot(d_{t}-d_{t+1}),\qquad
r_{\text{col}}=-w_{\text{col}}\cdot\mathbf{1}[\text{collision}].
$$

Hence:
$$
r_t = r_{\text{pot}} + r_{\Delta d} + r_{\text{col}}.
$$

Note: the latter two terms are non-potential additions; they intentionally trade policy-invariance guarantees for stronger practical shaping and safety pressure.

Implementation: `auto-shaping/auto_shaping/hprs_vec_wrapper.py`

## 5. Constrained LLM Outer-Loop Optimization

### 5.1 Segment-Level Summarization
After each online segment, metrics are summarized from `monitor.csv`:
$$
\text{success\_rate},\;
\text{collision\_rate},\;
\text{mean\_reward},\;
\text{mean\_success\_reward},\;
\text{mean\_length}.
$$

Implementation: `agent/tools/llm_summarize_run.py`

### 5.2 Patch Proposal and Constraints
The LLM proposes updates to a restricted subset of HPRS constants, with bounded change magnitude and parameter-level constraints (including recently-changed avoidance).

Implementation: `agent/tools/llm_propose_patch.py`, `agent/tools/run_llm_pipeline.py`, `agent/tools/llm_apply_patch.py`

### 5.3 Acceptance Criterion
Given old/new metrics $(sr_o,cr_o,r_o)$ and $(sr_n,cr_n,r_n)$, with tolerances $(\delta_s,\delta_c,\delta_r)$:
$$
\text{success\_not\_worse}: sr_n \ge sr_o-\delta_s,
$$
$$
\text{collision\_not\_worse}: cr_n \le cr_o+\delta_c.
$$
If success and collision are effectively unchanged:
$$
|sr_n-sr_o|\le \delta_s\;\land\;|cr_n-cr_o|\le \delta_c
\Rightarrow r_n \ge r_o+\delta_r.
$$
A patch is accepted iff all required conditions hold; otherwise it is rejected and previous HPRS is retained.

Implementation: `agent/tools/run_online_llm_loop.py`

## 6. Reproducibility Configuration
- Offline training config: `agent/config/offline_td3_bc.yaml`
- Online baseline config: `agent/config/online_td3_baseline.yaml`
- Online LLM-HPRS config: `agent/config/online_td3_llm.yaml`
- Comparative evaluation config: `agent/config/evaluate_compare_models.yaml`

## 7. Method-Level Contributions
1. A practical offline-to-online AGV navigation pipeline with strong initialization and online adaptation.  
2. VecEnv-level hierarchical shaping that is minimally invasive to the simulator backend.  
3. A constrained, auditable, rollback-safe LLM outer loop for reward-constant optimization.  
4. Explicit acceptance rules coupling task success and safety metrics.  
